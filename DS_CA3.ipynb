{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DS CA3\n",
    "## Task 1\n",
    "Written by Babak Hosseini Mohtasham 810101408, Parsa Ahmadi 810101609 and Mahdy Naeini 810101536\\\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from IPython.display import display, HTML\n",
    "display(HTML('<style>pre { white-space: pre !important; }</style>'))\n",
    "from pyspark.sql import functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-Up!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_df(df,n=5):\n",
    "    print(f'Count of data frame is: {df.count()}')\n",
    "    df.show(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/11 14:33:59 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    }
   ],
   "source": [
    "stocks_spark = SparkSession.builder.appName('stocks').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = stocks_spark.read.csv('stocks.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of data frame is: 1762\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- Open: double (nullable = true)\n",
      " |-- High: double (nullable = true)\n",
      " |-- Low: double (nullable = true)\n",
      " |-- Close: double (nullable = true)\n",
      " |-- Volume: integer (nullable = true)\n",
      " |-- Adj Close: double (nullable = true)\n",
      "\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|summary|              Open|              High|               Low|            Close|             Volume|         Adj Close|\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "|  count|              1762|              1762|              1762|             1762|               1762|              1762|\n",
      "|   mean| 313.0763111589103| 315.9112880164581| 309.8282405079457|312.9270656379113|9.422577587968218E7| 75.00174115607275|\n",
      "| stddev|185.29946803981522|186.89817686485767|183.38391664371008|185.1471036170943|6.020518776592709E7| 28.57492972179906|\n",
      "|    min|              90.0|         90.699997|         89.470001|        90.279999|           11475900|         24.881912|\n",
      "|    max|        702.409988|        705.070023|        699.569977|       702.100021|          470249500|127.96609099999999|\n",
      "+-------+------------------+------------------+------------------+-----------------+-------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df(df_pyspark)\n",
    "df_pyspark.printSchema()\n",
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of data frame is: 1762\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df(df_pyspark.drop()) #No null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of data frame is: 1359\n",
      "+----------+------------------+---------+\n",
      "|      Open|             Close|   Volume|\n",
      "+----------+------------------+---------+\n",
      "|213.429998|        214.009998|123432400|\n",
      "|214.599998|        214.379993|150476200|\n",
      "|214.379993|        210.969995|138040000|\n",
      "|    211.75|            210.58|119282800|\n",
      "|210.299994|211.98000499999998|111902700|\n",
      "+----------+------------------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df(df_pyspark.filter(df_pyspark['Close'] < 500).select(['Open','Close','Volume']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of data frame is: 3\n",
      "+------------------+----------+---------+\n",
      "|              Open|     Close|   Volume|\n",
      "+------------------+----------+---------+\n",
      "|206.78000600000001|    197.75|220441900|\n",
      "|        204.930004|199.289995|293375600|\n",
      "|        201.079996|192.060003|311488100|\n",
      "+------------------+----------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df(df_pyspark.filter((df_pyspark['Open'] > 200) & (df_pyspark['Close'] < 200)).select(['Open', 'Close', 'Volume']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of data frame is: 1762\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|      Date|      Open|      High|               Low|             Close|   Volume|         Adj Close|Year|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "|2010-01-04|213.429998|214.499996|212.38000099999996|        214.009998|123432400|         27.727039|2010|\n",
      "|2010-01-05|214.599998|215.589994|        213.249994|        214.379993|150476200|27.774976000000002|2010|\n",
      "|2010-01-06|214.379993|    215.23|        210.750004|        210.969995|138040000|27.333178000000004|2010|\n",
      "|2010-01-07|    211.75|212.000006|        209.050005|            210.58|119282800|          27.28265|2010|\n",
      "|2010-01-08|210.299994|212.000006|209.06000500000002|211.98000499999998|111902700|         27.464034|2010|\n",
      "+----------+----------+----------+------------------+------------------+---------+------------------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Year', functions.year(df_pyspark['Date']))\n",
    "show_df(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of data frame is: 7\n",
      "+----+-----------+\n",
      "|Year|min(Volume)|\n",
      "+----+-----------+\n",
      "|2015|   13046400|\n",
      "|2013|   41888700|\n",
      "|2014|   14479600|\n",
      "|2012|   43938300|\n",
      "|2016|   11475900|\n",
      "+----+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df(df_pyspark.groupBy('Year').min(('Volume')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of data frame is: 7\n",
      "+----+---------+\n",
      "|Year|minVolume|\n",
      "+----+---------+\n",
      "|2015| 13046400|\n",
      "|2013| 41888700|\n",
      "|2014| 14479600|\n",
      "|2012| 43938300|\n",
      "|2016| 11475900|\n",
      "|2010| 39373600|\n",
      "|2011| 44915500|\n",
      "+----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df(df_pyspark.groupBy('Year').min('Volume').withColumnRenamed('min(Volume)', 'minVolume'),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of data frame is: 84\n",
      "+----+-----+------------------+\n",
      "|Year|Month|            maxLow|\n",
      "+----+-----+------------------+\n",
      "|2012|   10|        665.550026|\n",
      "|2010|    7|        260.300003|\n",
      "|2010|   12|        325.099991|\n",
      "|2015|    2|        131.169998|\n",
      "|2014|    4|        589.799988|\n",
      "|2015|   12|        117.809998|\n",
      "|2016|    7|            103.68|\n",
      "|2016|   11|        111.400002|\n",
      "|2012|    8| 673.5400089999999|\n",
      "|2013|    2|473.24997699999994|\n",
      "|2012|    4| 626.0000150000001|\n",
      "|2012|   12|        585.500023|\n",
      "|2014|   10|        107.209999|\n",
      "|2016|    5|             99.25|\n",
      "|2014|   12|        115.290001|\n",
      "|2013|    9|        503.479988|\n",
      "|2013|   10|        525.110016|\n",
      "|2014|    5|        628.900002|\n",
      "|2016|    2|         96.650002|\n",
      "|2013|   12| 566.4100269999999|\n",
      "|2014|    1|        552.020004|\n",
      "|2010|   11|        316.759987|\n",
      "|2011|    3|        357.750004|\n",
      "|2013|    3|        461.780022|\n",
      "|2014|    8|        102.199997|\n",
      "|2013|    6|447.38999900000005|\n",
      "|2010|    6|        271.499992|\n",
      "|2012|    7| 605.9999849999999|\n",
      "|2012|    9|        699.569977|\n",
      "|2015|    4|        131.149994|\n",
      "|2010|    5|        262.880009|\n",
      "|2015|    8|        117.519997|\n",
      "|2016|    9|        114.040001|\n",
      "|2010|    9|        291.009998|\n",
      "|2011|    2|             360.5|\n",
      "|2011|    4|        350.300007|\n",
      "|2011|   12|        403.490009|\n",
      "|2015|   11|        121.620003|\n",
      "|2011|    8|        392.369995|\n",
      "|2016|   10|        117.449997|\n",
      "|2015|    9|        115.440002|\n",
      "|2013|    7|449.42999299999997|\n",
      "|2014|    9|        102.720001|\n",
      "|2013|    4|        432.069996|\n",
      "|2014|    3| 539.5899730000001|\n",
      "|2015|   10|        119.449997|\n",
      "|2016|    6|         98.959999|\n",
      "|2013|   11|        547.809975|\n",
      "|2011|    5|        346.880009|\n",
      "|2013|    5|        455.810005|\n",
      "|2011|    7|399.67998900000003|\n",
      "|2010|    8|        260.549995|\n",
      "|2016|   12|        116.779999|\n",
      "|2014|    2| 545.6099780000001|\n",
      "|2012|   11|        594.170021|\n",
      "|2016|    1|        102.410004|\n",
      "|2016|    4|111.33000200000001|\n",
      "|2014|    6|        644.470024|\n",
      "|2011|    6|        344.649998|\n",
      "|2010|   10|        314.289997|\n",
      "|2015|    3|        128.320007|\n",
      "|2010|    2|        202.000004|\n",
      "|2010|    3|        234.459999|\n",
      "|2011|   11|        401.560005|\n",
      "|2012|    1|453.07002300000005|\n",
      "|2015|    7|        130.699997|\n",
      "|2012|    2|        535.700005|\n",
      "|2014|   11|        118.050003|\n",
      "|2015|    6|        130.050003|\n",
      "|2010|    4|268.19001000000003|\n",
      "|2012|    3| 610.3099900000001|\n",
      "|2011|   10|        415.990002|\n",
      "|2012|    5| 581.2300190000001|\n",
      "|2013|    1| 541.6300200000001|\n",
      "|2013|    8|             504.0|\n",
      "|2016|    8|        109.209999|\n",
      "|2015|    1|        116.849998|\n",
      "|2011|    1|        344.440006|\n",
      "|2015|    5|        131.399994|\n",
      "|2010|    1|        213.249994|\n",
      "|2012|    6|        583.100021|\n",
      "|2011|    9|        412.000004|\n",
      "|2014|    7|             98.25|\n",
      "|2016|    3|        108.879997|\n",
      "+----+-----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = df_pyspark.withColumn('Month', functions.month(df_pyspark['Date']))\n",
    "show_df(df_pyspark.groupBy(['Year', 'Month']).max('Low').withColumnRenamed('max(Low)', 'maxLow'), 84)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+------------------+\n",
      "|summary|              Year|            Month|            maxLow|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "|  count|                84|               84|                84|\n",
      "|   mean|            2013.0|              6.5| 330.9219059404761|\n",
      "| stddev|2.0120121200142433|3.472785764174839|195.21157232941835|\n",
      "|    min|              2010|                1|         96.650002|\n",
      "|    max|              2016|               12|        699.569977|\n",
      "+-------+------------------+-----------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy(['Year', 'Month']).max('Low').withColumnRenamed('max(Low)', 'maxLow').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('spotify').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.parquet('spotify.parquet', header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- album: string (nullable = true)\n",
      " |-- album_id: string (nullable = true)\n",
      " |-- artists: string (nullable = true)\n",
      " |-- artist_ids: string (nullable = true)\n",
      " |-- track_number: long (nullable = true)\n",
      " |-- disc_number: long (nullable = true)\n",
      " |-- explicit: boolean (nullable = true)\n",
      " |-- danceability: double (nullable = true)\n",
      " |-- energy: double (nullable = true)\n",
      " |-- key: long (nullable = true)\n",
      " |-- loudness: double (nullable = true)\n",
      " |-- mode: long (nullable = true)\n",
      " |-- speechiness: double (nullable = true)\n",
      " |-- acousticness: double (nullable = true)\n",
      " |-- instrumentalness: double (nullable = true)\n",
      " |-- liveness: double (nullable = true)\n",
      " |-- valence: double (nullable = true)\n",
      " |-- tempo: double (nullable = true)\n",
      " |-- duration_ms: long (nullable = true)\n",
      " |-- time_signature: double (nullable = true)\n",
      " |-- year: long (nullable = true)\n",
      " |-- release_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of data frame is: 1204025\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----------+--------+------------+------+---+------------------+----+-----------+------------+----------------+--------+-------+-------+-----------+--------------+----+------------+\n",
      "|                  id|                name|               album|            album_id|             artists|          artist_ids|track_number|disc_number|explicit|danceability|energy|key|          loudness|mode|speechiness|acousticness|instrumentalness|liveness|valence|  tempo|duration_ms|time_signature|year|release_date|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----------+--------+------------+------+---+------------------+----+-----------+------------+----------------+--------+-------+-------+-----------+--------------+----+------------+\n",
      "|7lmeHLHBe4nmXzuXc...|             Testify|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           1|          1|   false|        0.47| 0.978|  7|            -5.399|   1|     0.0727|      0.0261|         1.09E-5|   0.356|  0.503|117.906|     210133|           4.0|1999|  1999-11-02|\n",
      "|1wsRitfRRtWyEapl0...|     Guerrilla Radio|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           2|          1|    true|       0.599| 0.957| 11|-5.763999999999999|   1|      0.188|      0.0129|         7.06E-5|   0.155|  0.489| 103.68|     206200|           4.0|1999|  1999-11-02|\n",
      "|1hR0fIFK2qRG3f3RF...|    Calm Like a Bomb|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           3|          1|   false|       0.315|  0.97|  7|            -5.424|   1|      0.483|      0.0234|         2.03E-6|   0.122|   0.37|149.749|     298893|           4.0|1999|  1999-11-02|\n",
      "|2lbASgTSoDO7MTuLA...|           Mic Check|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           4|          1|    true|        0.44| 0.967| 11|             -5.83|   0|      0.237|       0.163|         3.64E-6|   0.121|  0.574| 96.752|     213640|           4.0|1999|  1999-11-02|\n",
      "|1MQTmpYOZ6fcMQc56...|Sleep Now In the ...|The Battle Of Los...|2eia0myWFgoHuttJy...|['Rage Against Th...|['2d0hyoQ5ynDBnkv...|           5|          1|   false|       0.426| 0.929|  2|            -6.729|   1|     0.0701|     0.00162|           0.105|  0.0789|  0.539|127.059|     205600|           4.0|1999|  1999-11-02|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+--------------------+------------+-----------+--------+------------+------+---+------------------+----+-----------+------------+----------------+--------+-------+-------+-----------+--------------+----+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_df(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of data frame is: 1204025\n",
      "+------------+------------+\n",
      "|release_date|release_time|\n",
      "+------------+------------+\n",
      "|  1999-11-02|   941488200|\n",
      "|  1999-11-02|   941488200|\n",
      "|  1999-11-02|   941488200|\n",
      "|  1999-11-02|   941488200|\n",
      "|  1999-11-02|   941488200|\n",
      "|  1999-11-02|   941488200|\n",
      "|  1999-11-02|   941488200|\n",
      "|  1999-11-02|   941488200|\n",
      "|  1999-11-02|   941488200|\n",
      "|  1999-11-02|   941488200|\n",
      "|  1999-11-02|   941488200|\n",
      "|  1999-11-02|   941488200|\n",
      "|  1992-11-03|   720736200|\n",
      "|  1992-11-03|   720736200|\n",
      "|  1992-11-03|   720736200|\n",
      "|  1992-11-03|   720736200|\n",
      "|  1992-11-03|   720736200|\n",
      "|  1992-11-03|   720736200|\n",
      "|  1992-11-03|   720736200|\n",
      "|  1992-11-03|   720736200|\n",
      "+------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn('release_time', functions.to_unix_timestamp(df['release_date'], functions.lit('yyyy-MM-dd')))\n",
    "show_df(df[['release_date','release_time']],20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Read about how Spark and Hadoop work. What does the term ‘lazy evaluation’ mean\n",
    "for them? Explain with a simple example.**\\\n",
    "With Spark, only one-step is needed where data is read into memory, operations performed, and the results written back—resulting in a much faster execution. Spark also reuses data by using an in-memory cache to greatly speed up machine learning algorithms that repeatedly call a function on the same dataset.\\\n",
    "Hadoop is an open source framework based on Java that manages the storage and processing of large amounts of data for applications. Hadoop uses distributed storage and parallel processing to handle big data and analytics jobs, breaking workloads down into smaller workloads that can be run at the same time.\\\n",
    "Lazy evaluation is an evaluation strategy which holds the evaluation of an expression until its value is needed. It avoids repeated evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. **Your main task’s dataset has about 1,200,000 rows, which makes it quite hard, and even sometimes impossible, to work with. Explain how parquet files try to solve this problem, compared to normal file formats like csv.**\n",
    "Parquet file format is a columnar storage format, which means that data for each column is stored together. The storage mechanism enables better compression and typically results in smaller file sizes compared to row-based formats. CSV is a row-based format, where each row is represented as a separate line in the file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Parquet file format is a columnar storage format, which means that data for each column is stored together. The storage mechanism enables better compression and typically results in smaller file sizes compared to row-based formats. CSV is a row-based format, where each row is represented as a separate line in the file.**\n",
    "`DataFrame.checkpoint(eager=True)`: Returns a checkpointed version of this Dataset. Checkpointing can be used to truncate the logical plan of this DataFrame, which is especially useful in iterative algorithms where the plan may grow exponentially. It will be saved to files inside the checkpoint directory set with `SparkContext.setCheckpointDir()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. **Top companies stream their data on a regular routine, e.g. daily. How can we save data, so that we could filter it based on specific columns, e.g. date, faster than regular filtering?**\n",
    "A column-oriented DBMS or columnar DBMS is a database management system (DBMS) that stores data tables by column rather than by row. Benefits include more efficient access to data when only querying a subset of columns (by eliminating the need to read columns that are not relevant), and more options for data compression. However, they are typically less efficient for inserting new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. **Let's face off Pandas and PySpark in the data analysis arena! When does each library truly shine, and why? Consider factors like data size, processing complexity, and user experience**\n",
    "* **Ease of Use**: Pandas is generally easier to use and has a lower learning curve compared to PySpark. The pandas API is simple and the syntax is similar to SQL and Excel. This makes it easy for analysts and data scientists to get started with data analysis and manipulation using pandas.\n",
    "* **Interactivity**: Pandas provides an interactive environment for data exploration and analysis through Jupyter notebooks. This allows us to visualize data and experiment with code more easily. PySpark, on the other hand, can have a higher barrier to entry. It requires setting up a distributed computing cluster before running code.\n",
    "* **Well-suited for small to medium-sized data**: Pandas is well-suited for handling small to medium-sized datasets that can fit in memory. It provides fast and efficient data manipulation and processing on a single machine, without requiring distributed computing resources.\n",
    "* **Flexibility**: The pandas module is highly flexible and can work with a wide variety of data sources. We can use CSV, Excel, SQL databases, parquet files, and more. It also provides a wide range of data manipulation functions that can handle complex data transformation tasks.\n",
    "* **Integration with Other Libraries**: Pandas integrates well with other data science libraries in the Python ecosystem, such as NumPy, Matplotlib, and Scikit-learn. This makes it easy to build end-to-end data analysis pipelines and machine learning workflows using a variety of tools.\n",
    "* **Community Support**: Pandas has a large and active community of users and contributors. It also has extensive documentation that explains each function with examples. This makes it easy to find help and resources when working with Pandas.\n",
    "* **Scalability**: PySpark is designed to handle large-scale datasets and distributed computing. Using pyspark, we can perform parallel processing across a cluster of machines. We can split data into smaller partitions and perform parallel processing on them. This makes pyspark faster and more efficient than Pandas for large-scale data processing.\n",
    "* **Distributed Computing**: PySpark can distribute computations across a cluster of machines. This helps us process large-scale data that may not fit into the memory of a single machine. Due to this, PySpark is ideal for big data processing.\n",
    "* **Speed**: PySpark is faster than Pandas when processing large datasets. It can leverage the computing power of a cluster of machines to perform parallel processing. This can significantly reduce processing times.\n",
    "* **Integration with Big Data Tools**: PySpark integrates with a wide range of big data tools and technologies, including Hadoop, Hive, Cassandra, and HBase. This makes it easier to work with large datasets stored in distributed file systems and other big data stores.\n",
    "* **Integration with Hadoop Ecosystem**: PySpark integrates seamlessly with the Hadoop ecosystem. This enables us to work with data stored in Hadoop Distributed File System (HDFS) and other data sources such as HBase, Hive, and Cassandra.\n",
    "* **Streaming Data Processing**: PySpark Streaming allows users to process real-time data streams using Spark’s distributed computing capabilities. It can ingest data from various sources, including Kafka, Flume, and Twitter, and process them in near real-time. Pandas doesn’t have any such feature."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
